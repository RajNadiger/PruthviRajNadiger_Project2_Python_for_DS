{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b3d1a6",
   "metadata": {},
   "source": [
    "\n",
    "    ## Section 1: Import Libraries and Load Data\n",
    "    This section imports necessary libraries and loads the dataset, preparing it for analysis. We begin by importing libraries essential for data manipulation, visualization, and machine learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('renttherunway.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283267d",
   "metadata": {},
   "source": [
    "\n",
    "    ## Section 2: Data Cleansing and Exploratory Data Analysis\n",
    "    Here, we clean the data by removing duplicates and unnecessary columns, and conduct exploratory data analysis to understand the dataset better. This involves checking the structure, removing redundant information, and preparing the data for further analysis.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810acf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['user_id', 'review_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weight'] = data['weight'].str.replace('lbs', '').astype(float)\n",
    "\n",
    "# data['rented_for'] = data['rented_for'].replace({'party: cocktail': 'party'})\n",
    "\n",
    "data['height'] = data['height'].apply(lambda x: float(str(x).split(' ')[0].replace(\"'\", ''))*12 + float(str(x).split(' ')[-1].replace('\"', '')))\n",
    "\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "data.fillna(data.mode().iloc[0], inplace=True)\n",
    "\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "data = data[(np.abs(stats.zscore(data[numeric_cols])) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c666a",
   "metadata": {},
   "source": [
    "\n",
    "    ## Section 3: Feature Engineering and Data Preparation\n",
    "    We add new features and prepare the dataset for modeling by encoding categorical variables and scaling numerical values. This is crucial for effective model performance, especially in clustering and PCA.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['weight_height_ratio'] = data['weight'] / data['height']\n",
    "data['review_month'] = pd.to_datetime(data['review_date']).dt.month\n",
    "\n",
    "le = LabelEncoder()\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_features:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c373af",
   "metadata": {},
   "source": [
    "\n",
    "    ## Section 4: Principal Component Analysis (PCA) and Clustering\n",
    "    Apply PCA to reduce dimensions and cluster the data using K-means and Agglomerative clustering methods. We will determine the optimal number of clusters using the elbow method and validate the clustering quality using the silhouette score.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b88848",
   "metadata": {},
   "source": [
    "\n",
    "    ## Conclusion\n",
    "    This section summarizes the findings from the clustering analysis, discussing how the clusters could be interpreted and suggesting potential strategies for customer segmentation based on the clustered data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Summary of clustering results and potential marketing strategies based on customer segments.\n",
    "    # (User should add detailed analysis and conclusions here based on the outputs from previous sections.)\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
